{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8085f465",
   "metadata": {},
   "source": [
    "[Original VQ-VAE paper](https://arxiv.org/pdf/1711.00937)\n",
    "\n",
    "The basic idea:\n",
    "\n",
    "- Take a normal-style VAE Decoder and Encoder\n",
    "- At the bottleneck, use a set of $K$ embedding vectors, $e_k$ of dimensionality $D$\n",
    "    - The vectors have a set latent dimension\n",
    "    - The vectors are learned during training\n",
    "- During inference, the latent vector output by the encoder is mapped to the nearest neighbour embedding vector\n",
    "- gradients flow back, with a straight-through estimator at the embedding layer\n",
    "- For image generation, we learn a 2D grid of embedded features\n",
    "\n",
    "\n",
    "The loss function is given by\n",
    "$$\n",
    "L \\;=\\; \\underbrace{\\log p\\bigl(x \\mid z_q(x)\\bigr)}_{\\text{Reconstruction Loss}}\n",
    "\\;+\\; \\underbrace{\\Bigl\\lVert \\mathrm{sg}\\bigl(z_e(x)\\bigr) - e \\Bigr\\rVert_{2}^{2}}_{\\text{Codebook Loss}}\n",
    "\\;+\\; \\underbrace{\\beta \\,\\Bigl\\lVert z_e(x) - \\mathrm{sg}(e)\\Bigr\\rVert_{2}^{2}}_{\\text{Commitment Loss}}.\n",
    "$$\n",
    "\n",
    "Where $sg(\\cdot)$ is the stopgradient operator. The three terms are:\n",
    "\n",
    "1. The Reconstruction Loss trains the encoder and decoder to generate close matches to the input image,\n",
    "2. The Codebook Loss moves the embedding vectors towards embeddings generated by the encoder,\n",
    "3. The Commitment Loss incentivises the encoder to keep latents close to the embedding vectors, rather than expanding out into the space in an unbounded manner.\n",
    "\n",
    "$\\beta$ = 0.25 is a good starting point. The given output shapes of the latent field are 32x32 (ImageNet) and 8x8x10 (Cifar10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a343ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from einops import rearrange\n",
    "from genlearn.data_utils import get_cifar_dataloaders\n",
    "import chex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32bde4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, valloader = get_cifar_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4d66ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 512\n",
    "D = 64  # embedding dimension\n",
    "h, w, c = (\n",
    "    8,\n",
    "    8,\n",
    "    10,\n",
    ")  # shape of the embedding space that vectors are projected into (32, 32, 1 for ImageNet)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42d2eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rngs = nnx.Rngs(0)\n",
    "codebook = rngs.normal((K, D))\n",
    "z = rngs.normal((batch_size, h, w, c, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5ac3b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40960, 64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_flattened = rearrange(z, \"b h w c d -> (b h w c) d\")\n",
    "z_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693be297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def embed(z, codebook):\n",
    "    # want to get the distance from every vector in z to every vector in the codebook\n",
    "    @partial(jax.vmap, in_axes=(0, None))\n",
    "    @partial(jax.vmap, in_axes=(None, 0))\n",
    "    def get_distance(e, z):\n",
    "        chex.assert_rank([e, z], 1)  # comparing 64-dimensional vectors\n",
    "        return jnp.linalg.norm(e - z)\n",
    "\n",
    "    # returns a batch * K matrix with all the distances\n",
    "    distances = get_distance(z, codebook)\n",
    "    nearest_embeddings_idx = jnp.argmin(distances, axis=1)\n",
    "    return codebook[nearest_embeddings_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6639a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_embedded = embed(z_flattened, codebook)\n",
    "z_embedded = rearrange(\n",
    "    z_embedded, \"(b h w c) d -> b h w c d\", b=batch_size, h=h, w=w, c=c\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fe1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnx.Module):\n",
    "    def __init__(self):\n",
    "        \"ResNet-style encoder, down to an 8x8x10x64 shape output\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class Decoder(nnx.Module):\n",
    "    def __init__(self):\n",
    "        \"ResNet-style decoder, promoting up from 8x8x10x64 latents back up to a 32x32x3 RGB out\"\n",
    "        ...\n",
    "\n",
    "\n",
    "class Embed(nnx.Module):\n",
    "    def __init__(self, K: int, D: int, *, rngs: nnx.Rngs):\n",
    "        \"Quantisation layer: converts the 8x8x10x64 latents to quantised representations using a learned codebook\"\n",
    "        self.codebook = nnx.Param(rngs.normal((K, D)))\n",
    "\n",
    "\n",
    "class VQ_VAE(nnx.Module):\n",
    "    def __init__(self):\n",
    "        self.encoder = Encoder()\n",
    "        self.embed = Embed()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def __call__(self, x_bhwc):\n",
    "        ze_bwhcd = self.encoder(x_bhwc)\n",
    "        e_bwhcd = self.embed(ze_bwhcd)\n",
    "        zq_bhwc = self.decoder(e_bwhcd)\n",
    "        return (\n",
    "            zq_bhwc,\n",
    "            e_bwhcd,\n",
    "            ze_bwhcd,\n",
    "        )  # reconstruction, the quantised vectors, the encoded latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebe9c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in trainloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d46e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
