{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0328d5",
   "metadata": {},
   "source": [
    "Some simple illustrations of various calculations of increasing complexity using einsum. Inspired by Tim Röcktaschel's blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09d91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from einops import einsum\n",
    "\n",
    "\n",
    "def random_array(*shape, num: int = 1) -> jax.Array | list[jax.Array]:\n",
    "    key = jax.random.split(jax.random.key(0), num=num)\n",
    "    arr = [jax.random.uniform(key[i], shape=shape) for i in range(num)]\n",
    "    return arr[0] if num == 1 else arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "800967d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = random_array(4, 4, num=2)\n",
    "\n",
    "A @ B\n",
    "# jnp.einsum(\"ij,jk->ik\", A, B)\n",
    "\n",
    "assert jnp.allclose(einsum(A, B, 'i j, j k -> i k'), A @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ B.T\n",
    "jnp.einsum('ij,kj->ik', A, B)\n",
    "assert jnp.allclose(jnp.einsum('ij,kj->ik', A, B), A @ B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3075a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention has a term like X W_Q W_K X - can we do it as a one liner with einsum?\n",
    "B = 5  # batch size\n",
    "E = 10 # embedding dim\n",
    "D = 3  # hidden dim \n",
    "X = random_array(B, E)\n",
    "W_Q, W_K = random_array(E, D, num=2)\n",
    "\n",
    "v1 = X @ W_Q @ W_K.T @ X.T\n",
    "v2 = X @ W_Q @ (X @ W_K).T\n",
    "v3 = jnp.einsum('bi, ij, kj, lk -> bl', X, W_Q, W_K, X)\n",
    "assert jnp.allclose(v1, v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6926734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "from einops import einsum, rearrange\n",
    "from functools import partial\n",
    "from jax import vmap\n",
    "\n",
    "B = 32  # batch size\n",
    "T = 512  # seq length\n",
    "E = 128  # embedding dim\n",
    "D = 8  # hidden dim\n",
    "H = 16  # number of heads\n",
    "\n",
    "\n",
    "X_BTE = random_array(B, T, E)\n",
    "WQ_HED = random_array(H, E, D)\n",
    "WK_HED = random_array(H, E, D)\n",
    "WH_HEE = random_array(H, E, E) # combined value/output matrix across heads - technical full-rank so more expressive\n",
    "\n",
    "# used for vmap impl\n",
    "WV_HED = random_array(H, E, D)\n",
    "WO_GE = random_array(H * D, E)\n",
    "\n",
    "@jax.jit\n",
    "def attn_eins(X_BTE, WQ_HED, WK_HED, WH_HEE):\n",
    "    Q_BTHD = einsum(X_BTE, WQ_HED, 'b t e, h e d -> b t h d')\n",
    "    K_BTHD = einsum(X_BTE, WK_HED, 'b t e, h e d -> b t h d')\n",
    "    A_BHTT = einsum(Q_BTHD, K_BTHD, 'b tq h d, b tk h d -> b h tq tk')\n",
    "    A_BHTT = nnx.softmax(A_BHTT / jnp.sqrt(D), axis=-1)\n",
    "\n",
    "    # sum over heads and compute full-rank output\n",
    "    Y_BTE = einsum(A_BHTT, X_BTE, WH_HEE, 'b h tq tk, b tk e1, h e1 e2 -> b tq e2')\n",
    "    return Y_BTE\n",
    "\n",
    "attn_eins(X_BTE, WQ_HED, WK_HED, WH_HEE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0384f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now trying to do MHA with plenty of vmaps\n",
    "@jax.jit\n",
    "def attn_vmap(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE):\n",
    "    @partial(vmap, in_axes=(None, 0, 0, 0))       # over the head dimension\n",
    "    @partial(vmap, in_axes=(0, None, None, None)) # over the batch dimension\n",
    "    def _attn_2d(X_TE, WQ_ED, WK_ED, WV_ED):\n",
    "        Q_TD = X_TE @ WQ_ED\n",
    "        K_TD = X_TE @ WK_ED \n",
    "        V_TD = X_TE @ WV_ED\n",
    "\n",
    "        A_TT = nnx.softmax(Q_TD @ K_TD.T / jnp.sqrt(D), axis=-1)\n",
    "        H_TD = A_TT @ V_TD\n",
    "        return H_TD\n",
    "\n",
    "    H_HBTD = _attn_2d(X_BTE, WQ_HED, WK_HED, WV_HED)\n",
    "    H_BTD = rearrange(H_HBTD, 'h b t d -> b t (h d)')\n",
    "    Y_BTE = H_BTD @ WO_GE\n",
    "    return Y_BTE\n",
    "\n",
    "attn_vmap(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1091f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def attn_nnx(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE):\n",
    "    Q_BTHD = einsum(X_BTE, WQ_HED, 'b t e, h e d -> b t h d')\n",
    "    K_BTHD = einsum(X_BTE, WK_HED, 'b t e, h e d -> b t h d')\n",
    "    V_BTHE = einsum(X_BTE, WV_HED, 'b t e, h e d -> b t h d')\n",
    "    H_BTHD = nnx.dot_product_attention(Q_BTHD, K_BTHD, V_BTHE)\n",
    "\n",
    "    H_BTD = rearrange(H_BTHD, 'b t h d -> b t (h d)')\n",
    "    Y_BTE = H_BTD @ WO_GE\n",
    "    return Y_BTE\n",
    "\n",
    "attn_nnx(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 ms ± 1.42 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_nnx(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0855a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 ms ± 4.58 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_eins(X_BTE, WQ_HED, WK_HED, WH_HEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b62c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 ms ± 7.75 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_vmap(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f7656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def attn_fast_chatgpt(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE):\n",
    "    B, T, E = X_BTE.shape\n",
    "    H, _, D = WQ_HED.shape\n",
    "\n",
    "    # Pack heads -> 3 big GEMMs\n",
    "    WQ_p = WQ_HED.reshape(E, H * D)\n",
    "    WK_p = WK_HED.reshape(E, H * D)\n",
    "    WV_p = WV_HED.reshape(E, H * D)\n",
    "\n",
    "    Q = (X_BTE @ WQ_p).reshape(B, T, H, D)  # (B,T,H,D)\n",
    "    K = (X_BTE @ WK_p).reshape(B, T, H, D)  # (B,T,H,D)\n",
    "    V = (X_BTE @ WV_p).reshape(B, T, H, D)  # (B,T,H,D)\n",
    "\n",
    "    # Explicit attention (no lax SDPA)\n",
    "    scale = 1.0 / jnp.sqrt(D)\n",
    "    scores = jnp.einsum('b t h d, b s h d -> b h t s', Q, K) * scale   # (B,H,T,S)\n",
    "    probs  = jax.nn.softmax(scores, axis=-1)                           # (B,H,T,S)\n",
    "    H_out  = jnp.einsum('b h t s, b s h d -> b t h d', probs, V)       # (B,T,H,D)\n",
    "\n",
    "    Y_BTE = H_out.reshape(B, T, H * D) @ WO_GE                          # (B,T,E)\n",
    "    return Y_BTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243f8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def attn_eins(X_BTE, WQ_HED, WK_HED, WH_HEE):\n",
    "    Q_BTHD = einsum(X_BTE, WQ_HED, 'b t e, h e d -> b t h d')\n",
    "    K_BTHD = einsum(X_BTE, WK_HED, 'b t e, h e d -> b t h d')\n",
    "    A_BHTT = einsum(Q_BTHD, K_BTHD, 'b tq h d, b tk h d -> b h tq tk')\n",
    "    A_BHTT = nnx.softmax(A_BHTT / jnp.sqrt(D), axis=-1)\n",
    "\n",
    "    # sum over heads and compute full-rank output\n",
    "    Y_BTE = einsum(A_BHTT, X_BTE, WH_HEE, 'b h tq tk, b tk e1, h e1 e2 -> b tq e2')\n",
    "    return Y_BTE\n",
    "\n",
    "attn_eins(X_BTE, WQ_HED, WK_HED, WH_HEE);\n",
    "\n",
    "@jax.jit\n",
    "def attn_eins2(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE):\n",
    "    # G ~ H * D\n",
    "    WQ_EG = rearrange(WQ_HED, 'h e d -> e (h d)')\n",
    "    WK_EG = rearrange(WK_HED, 'h e d -> e (h d)')\n",
    "    WV_EG = rearrange(WV_HED, 'h e d -> e (h d)')\n",
    "\n",
    "    Q_BTHD = rearrange(X_BTE @ WQ_EG, 'b t (h d) -> b t h d', h=H)\n",
    "    K_BTHD = rearrange(X_BTE @ WK_EG, 'b t (h d) -> b t h d', h=H)\n",
    "    V_BTHD = rearrange(X_BTE @ WV_EG, 'b t (h d) -> b t h d', h=H)\n",
    "    A_BHTT = einsum(Q_BTHD, K_BTHD, 'b tq h d, b tk h d -> b h tq tk')\n",
    "    A_BHTT = nnx.softmax(A_BHTT / jnp.sqrt(D), axis=-1)\n",
    "\n",
    "    H_BTHD = einsum(A_BHTT, V_BTHD, 'b h tq tk, b tk h d -> b tq h d')\n",
    "    H_BTG = rearrange(H_BTHD, 'b t h d -> b t (h d)')\n",
    "    return H_BTG @ WO_GE\n",
    "\n",
    "attn_eins2(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE);\n",
    "\n",
    "@jax.jit\n",
    "def attn_eins3(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE):\n",
    "    Q_BTHD = einsum(X_BTE, WQ_HED, 'b t e, h e d -> b t h d')\n",
    "    K_BTHD = einsum(X_BTE, WK_HED, 'b t e, h e d -> b t h d')\n",
    "    V_BTHD = einsum(X_BTE, WV_HED, 'b t e, h e d -> b t h d')\n",
    "\n",
    "    A_BHTT = einsum(Q_BTHD, K_BTHD, 'b tq h d, b tk h d -> b h tq tk')\n",
    "    A_BHTT = nnx.softmax(A_BHTT / jnp.sqrt(D), axis=-1)\n",
    "\n",
    "    H_BTHD = einsum(A_BHTT, V_BTHD, 'b h tq tk, b tk h d -> b tq h d')\n",
    "    H_BTG = rearrange(H_BTHD, 'b t h d -> b t (h d)')\n",
    "    return H_BTG @ WO_GE\n",
    "\n",
    "attn_eins3(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2342fc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 ms ± 1.92 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_eins(X_BTE, WQ_HED, WK_HED, WH_HEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "942c087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 ms ± 1.42 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_eins2(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d8d5bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 ms ± 2.81 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_eins3(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_fast_chatgpt(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 ms ± 1.37 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "attn_nnx(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c3fba40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing batched GEMMs\n",
    "M1_ABCD = random_array(50, 50, 50, 50)\n",
    "M2_ABCD = random_array(50, 50, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8189bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mult_1(m1, m2):\n",
    "    return einsum(m1, m2, 'a b i j, a b j k -> a b i k')\n",
    "\n",
    "@jax.jit\n",
    "def mult_2(m1, m2):\n",
    "    return einsum(m1, m2, 'i j a b, j k a b -> i k a b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f0dc8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.25 ms ± 33.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = mult_1(M1_ABCD, M2_ABCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dde5bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.7 ms ± 60 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = mult_2(M1_ABCD, M2_ABCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50518375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch comparison\n",
    "x_torch = torch.from_dlpack(X_BTE)\n",
    "wq_torch = torch.from_dlpack(WQ_HED)\n",
    "wk_torch = torch.from_dlpack(WK_HED)\n",
    "wv_torch = torch.from_dlpack(WV_HED)\n",
    "wo_torch = torch.from_dlpack(WO_GE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91ad318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_torch(X_BTE, WQ_HED, WK_HED, WV_HED, WO_GE):\n",
    "    Q_BHTD = einsum(X_BTE, WQ_HED, 'b t e, h e d -> b h t d')\n",
    "    K_BHTD = einsum(X_BTE, WK_HED, 'b t e, h e d -> b h t d')\n",
    "    V_BHTE = einsum(X_BTE, WV_HED, 'b t e, h e d -> b h t d')\n",
    "    H_BHTD = F.scaled_dot_product_attention(Q_BHTD, K_BHTD, V_BHTE)\n",
    "\n",
    "    H_BTD = rearrange(H_BHTD, 'b h t d -> b t (h d)')\n",
    "    Y_BTE = H_BTD @ WO_GE\n",
    "    return Y_BTE\n",
    "\n",
    "attn_torch(x_torch, wq_torch, wk_torch, wv_torch, wo_torch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a4c2ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.9 ms ± 1.61 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "attn_torch(x_torch, wq_torch, wk_torch, wv_torch, wo_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead7bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
